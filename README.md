# Phi4RagScience
Ein modulares, LLM- basiertes  CLI-System fÃ¼r wissenschaftliche Recherche (pdf) und Dialoganalyse.
# Science Dialog Ensemble CLI

Ein modulares CLI-System fÃ¼r wissenschaftliche Recherche und Dialoganalyse.

## Features
- **Dialog-CLI** mit Short-Term-Memory (STM) und fortlaufendem **Plan** (Goals, Hypothesen, To-Dos).
- **Retrieval-Lanes**:
  - **Short**: Google (SerpAPI) â†’ schnelle Hinweise.
  - **Medium**: Wikipedia â†’ kompakte Primer.
  - **Slow**: arXiv dynamisch â†’ Abstracts + optionale **PDF-Chunking-Indizierung**.
- **Chunking-Plattform**: struktur-aware Chunking + Hybrid-Retrieval (BM25 + FAISS + RRF + MMR).
- **Understanding-Layer** (Mini-LLM): Facetten/Synonyme/Variablen â†’ Subqueries; Vetting (Coverage, Semantik, optional NLI, optional Mini-LLM-Rescore).
- **Tandem-Antwort**: **Phi-4 Mini lokal** + **OpenAI** (konfigurierbar: phi4_only, openai_only, parallel).
- **Claimâ†’Evidence Filter** + **Metriken** (Support-Rate, Wilson-Lower-Bound).


Hier ist das vollstÃ¤ndige, abgestimmte, lauffÃ¤hige Gesamt-System mit Doppel-LLM, Dialog-CLI, STM, Plan, Lanes (Short/Medium/Slow), Chunking-Plattform, Understanding-Layer, Orchestrator und Haupt-CLI.


## Installation
pip install -r requirements.txt
OpenAI-Key: als Umgebungsvariable OPENAI_API_KEY (oder .env).


# 1) Dialog (STM + Plan)
python science_cli_ensemble.py dialog

# 2) Einzelschuss-Chat (ohne Plan, ein Turn)
python science_cli_ensemble.py chat --query "Vergleich Self-Attention-Varianten 2024/25"

# 3) arXiv-Suche + optional PDF-Chunking
python science_cli_ensemble.py search --query "graph neural networks" --download-top 2

# 4) Indizes ansehen / neu bauen
python science_cli_ensemble.py show-index
python science_cli_ensemble.py reindex

# 5) Smoke-Test
python science_cli_ensemble.py selftest



#Zusammenfassung
Du hast jetzt ein vollstÃ¤ndiges, lauffÃ¤higes System mit:
Dialog-CLI (STM + Plan),
Short/Medium/Slow-Lanes inkl. dynamischer arXiv + PDF-Chunking,
Chunking-Plattform (strukturbewusst, Hybrid-Retrieval),
Understanding-Layer (Plan, Subqueries, Vetting),
Orchestrator,
Doppel-LLM (Phi-4 lokal + OpenAI),
Claimâ†’Evidence-Filter und Metriken,
Entry-Point science_cli_ensemble.py.

#Hinweise:

Phi-4 lokal: LÃ¤dt ohne 4-bit, damit keine zusÃ¤tzliche AbhÃ¤ngigkeit (bitsandbytes) nÃ¶tig ist. Auf CPU langsamer; wennâ€™s kneift, stell in config.json mode: "openai_only".


SerpAPI ist optional (Short-Lane). Wikipedia ist standardmÃ¤ÃŸig an.
PDF-Chunking ist standardmÃ¤ÃŸig an (policy: "chunked"). Du kannst es abschalten oder auf â€žfullâ€œ stellen (nicht empfohlen auf 8 GB).
Wenn du magst, kann ich dir noch eine minimale config.json hier ablegen â€“ sag Bescheid, dann pack ichâ€™s sofort dazu.

---

# ðŸ“„ requirements.txt
sentence-transformers
faiss-cpu
rank-bm25
feedparser
requests
wikipedia
openai>=1.0.0
pdfplumber
torch
transformers>=4.40.0
python-dotenv

#pip install -r requirements.txt
#Ordner (werden bei Bedarf automatisch angelegt):
./abstracts   ./pdfs   ./chunks

#Erster Test

# Dynamische arXiv-Suche + 1 PDF indizieren
python science_cli_ensemble.py search --query "graph neural networks" --download-top 1

# Einzelschuss
python science_cli_ensemble.py chat --query "ErklÃ¤re kurz Backpropagation."

# Dialog (STM + Plan)
python science_cli_ensemble.py dialog

#NÃ¼tzliche Umschalter (fallsâ€™s hakt)

Nur OpenAI (schnell, wenig RAM): "mode": "openai_only"
Kein PDF-Download: "pdf": { "policy": "chunked", "top_k": 0 }
Wikipedia aus: "lanes": { "wikipedia": false, ... }
NLI prÃ¤ziser machen: "understanding": { "nli": true } (langsamer)

#Wenn du willst, gebâ€™ ich dir noch eine zweite config.openai-only.json und config.cpu.json (konservativ fÃ¼r 8 GB) dazu.

#config.openai-only.json
{
  "ensemble": {
    "mode": "openai_only",
    "openai_model": "gpt-4o-mini",
    "phi4_model": "microsoft/Phi-4-mini-instruct",
    "max_new_tokens": 220
  },
  "lanes": {
    "wikipedia": true,
    "serpapi": false,
    "serpapi_key": ""
  },
  "rag": {
    "threshold": 0.93,
    "max_results": 12
  },
  "pdf": {
    "policy": "chunked",
    "top_k": 1
  },
  "understanding": {
    "enable": true,
    "nli": false,
    "llm_rescore_top": 2
  },
  "context": {
    "top_chunks": 8
  },
  "paths": {
    "abstracts": "./abstracts",
    "pdfs": "./pdfs",
    "chunks": "./chunks"
  }
}

#config.cpu-conservative.json (8-GB-Laptop, maximal sparsam)
{
  "ensemble": {
    "mode": "openai_only",
    "openai_model": "gpt-4o-mini",
    "phi4_model": "microsoft/Phi-4-mini-instruct",
    "max_new_tokens": 180
  },
  "lanes": {
    "wikipedia": true,
    "serpapi": false,
    "serpapi_key": ""
  },
  "rag": {
    "threshold": 0.94,
    "max_results": 10
  },
  "pdf": {
    "policy": "chunked",
    "top_k": 0
  },
  "understanding": {
    "enable": true,
    "nli": false,
    "llm_rescore_top": 1
  },
  "context": {
    "top_chunks": 6
  },
  "paths": {
    "abstracts": "./abstracts",
    "pdfs": "./pdfs",
    "chunks": "./chunks"
  }
}

#Wenn du spÃ¤ter Phi-4 lokal zuschalten willst: in config.json ensemble.mode auf "parallel" stellen.

FÃ¼r SerpAPI-Short-Lane: lanes.serpapi=true und lanes.serpapi_key setzen.
Wenn Antworten zu lang werden: ensemble.max_new_tokens weiter runter (150â€“180).

#1. Einzelschuss-Modus (wie `chat --query "...")

Jede Frage wird per CLI-Argument Ã¼bergeben.
Kein Short-Term-Memory, kein Plan.
Praktisch fÃ¼r schnelle Tests oder Skripte.
#Beispiel:
python science_cli_ensemble.py chat --query "Was ist Backpropagation?"

#2. Dialog-Modus (Subcommand dialog)

Startest eine REPL (= Read-Eval-Print Loop).
Du bleibst im Prompt, tippst nacheinander deine Fragen.
System merkt sich den STM (z. B. letzte 6 Turns) und pflegt den Plan (plan.json).
Du kannst zwischendurch Kommandos nutzen wie:

:plan â†’ aktuellen Plan anzeigen
:ctx â†’ aktuellen Kurzzeit-Kontext ansehen
:save / :load â†’ Plan persistieren oder laden
:reset â†’ Dialog-Kontext lÃ¶schen

#Beispiel:

python science_cli_ensemble.py dialog

#Dann:

> ErklÃ¤r mir kurz Self-Attention
[Antwort]

> Und was ist die RechenkomplexitÃ¤t?
[Antwort]

> :plan
[Zeigt Goals/Hypothesen/Todos]

Fazit

ðŸ‘‰ Ein echter Dialog ist mÃ¶glich, wenn du mit dialog startest.
ðŸ‘‰ Wenn du nur chat --query nutzt, musst du jede Frage separat aufrufen.

python science_cli_ensemble.py dialog

#Was du im Dialog machen kannst

Einfach Fragen tippen â€“ STM (KurzzeitgedÃ¤chtnis) + Plan laufen mit.
Befehle:
:plan â€“ aktuellen Forschungs-Plan zeigen (goals/hypotheses/todos/notes/sources)
:ctx â€“ Kurzzeit-Kontext (letzte ~6 Turns) anzeigen
:save / :load â€“ Plan speichern/laden (./plan.json)
:reset â€“ STM & Plan zurÃ¼cksetzen
Ctrl+C â€“ beenden

Vorher kurz prÃ¼fen
config.json passt (z. B. ensemble.mode: "parallel" oder "openai_only").
OPENAI_API_KEY gesetzt (fÃ¼r die OpenAI-Lane).
Optional: lanes.serpapi=true + lanes.serpapi_key fÃ¼r die Short-Lane.

#Wie duâ€™s nutzt:

Konsole + Datei-Log:
python science_cli_ensemble.py --log-level DEBUG --log-file sci.log dialog
â†’ Konsole: kompakt. Datei sci.log: detailliert (mit Datei/Zeile).

#test-stapel
py hello_session.py
# â†’ schreibt detailierten Log nach hello.log und zeigt Antworten/Metriken an

Wenn du spÃ¤ter noch feinere Traces auf Modul-Ebene willst, kann ich zusÃ¤tzlich in orchestrator.py, lanes.py, chunking_platform.py und understanding_layer.py interne log.debug(...)-Statements einziehen (z. B. pro Subquery/Ranking/Score). FÃ¼r jetzt bekommst du den vollen Ablauf sichtbar â€“ ohne die anderen Module zu verbiegen.
